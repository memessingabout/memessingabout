import os
import json
import hashlib
import hmac
import base64
from datetime import datetime, timedelta
from time import sleep
import sqlite3
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('uber_scraper.log'),
        logging.StreamHandler()
    ]
)

class UberDataScraper:
    def __init__(self, encryption_key=None):
        self.driver = None
        self.encryption_key = encryption_key or self._generate_encryption_key()
        self.db_path = 'uber_trips_data.db'
        self._init_database()
        
    def _generate_encryption_key(self):
        """Generate a secure encryption key"""
        return base64.b64encode(os.urandom(32)).decode('utf-8')
    
    def _init_database(self):
        """Initialize SQLite database with encrypted storage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS trips (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                trip_id TEXT UNIQUE,
                encrypted_data BLOB,
                data_hash TEXT,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                week_start DATE
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scraping_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                weeks_processed INTEGER,
                trips_collected INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _encrypt_data(self, data):
        """Encrypt trip data using HMAC for integrity and simple obfuscation"""
        json_data = json.dumps(data, ensure_ascii=False)
        data_hash = hmac.new(
            self.encryption_key.encode('utf-8'),
            json_data.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()
        
        # Simple base64 encoding for obfuscation (for production, use proper encryption)
        encrypted = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')
        return encrypted, data_hash
    
    def _decrypt_data(self, encrypted_data, expected_hash):
        """Decrypt and verify data integrity"""
        try:
            json_data = base64.b64decode(encrypted_data.encode('utf-8')).decode('utf-8')
            
            # Verify integrity
            actual_hash = hmac.new(
                self.encryption_key.encode('utf-8'),
                json_data.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            
            if actual_hash != expected_hash:
                raise ValueError("Data integrity check failed")
                
            return json.loads(json_data)
        except Exception as e:
            logging.error(f"Decryption error: {e}")
            return None
    
    def setup_browser(self, brave_path=None, profile_path=None):
        """Set up Brave browser with existing profile"""
        options = Options()
        
        # Set Brave browser path if provided
        if brave_path and os.path.exists(brave_path):
            options.binary_location = brave_path
        
        # Use existing profile if provided
        if profile_path and os.path.exists(profile_path):
            options.add_argument(f'--user-data-dir={profile_path}')
        else:
            # Default Brave profile location
            default_brave_profile = os.path.expanduser('~/.config/BraveSoftware/Brave-Browser/Default')
            if os.path.exists(default_brave_profile):
                options.add_argument(f'--user-data-dir={default_brave_profile}')
        
        options.add_argument('--no-first-run')
        options.add_argument('--no-default-browser-check')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        logging.info("Brave browser started with existing profile")
    
    def navigate_to_activities(self, uber_url):
        """Navigate to Uber driver activities page"""
        try:
            self.driver.get(uber_url)
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            logging.info("Successfully navigated to Uber activities page")
            sleep(3)  # Allow page to fully load
            return True
        except Exception as e:
            logging.error(f"Failed to navigate to activities page: {e}")
            return False
    
    def get_week_ranges(self, start_date=None, weeks_back=52):
        """Generate week ranges (Monday 4am to next Monday 3:59am)"""
        if not start_date:
            start_date = datetime.now()
        
        week_ranges = []
        for i in range(weeks_back):
            # Find most recent Monday
            days_since_monday = (start_date.weekday() - 0) % 7
            week_end = start_date - timedelta(days=days_since_monday)
            week_end = week_end.replace(hour=3, minute=59, second=59, microsecond=999999)
            week_start = week_end - timedelta(days=6, hours=23, minutes=59, seconds=59)
            
            week_ranges.append({
                'start': week_start,
                'end': week_end,
                'display': week_start.strftime('%Y-%m-%d')
            })
            
            start_date = week_start - timedelta(seconds=1)
        
        return week_ranges
    
    def navigate_to_week(self, week_date):
        """Navigate to specific week using calendar"""
        try:
            # Look for calendar element - this selector might need adjustment
            calendar_selector = "input[type='date'], input[type='text'][placeholder*='date'], [data-testid*='date']"
            calendar_input = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, calendar_selector))
            )
            
            # Clear and set date
            calendar_input.clear()
            calendar_input.send_keys(week_date.strftime('%Y-%m-%d'))
            
            # Wait for page to load new data
            sleep(5)
            logging.info(f"Navigated to week starting {week_date.strftime('%Y-%m-%d')}")
            return True
            
        except Exception as e:
            logging.error(f"Failed to navigate to week {week_date}: {e}")
            return False
    
    def load_all_trips_in_week(self):
        """Click 'Load More' button until all trips are loaded"""
        try:
            load_more_clicks = 0
            max_clicks = 50  # Safety limit
            
            while load_more_clicks < max_clicks:
                try:
                    # Look for load more button - these selectors might need adjustment
                    load_more_selectors = [
                        "button:contains('Load More')",
                        "button[data-testid*='load-more']",
                        "button[aria-label*='load more']",
                        "//button[contains(text(), 'Load More')]",
                        "//button[contains(text(), 'Load more')]"
                    ]
                    
                    load_more_button = None
                    for selector in load_more_selectors:
                        try:
                            if selector.startswith("//"):
                                load_more_button = self.driver.find_element(By.XPATH, selector)
                            else:
                                load_more_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                            break
                        except NoSuchElementException:
                            continue
                    
                    if load_more_button and load_more_button.is_enabled():
                        self.driver.execute_script("arguments[0].click();", load_more_button)
                        load_more_clicks += 1
                        logging.info(f"Clicked 'Load More' button ({load_more_clicks})")
                        sleep(2)  # Wait for content to load
                    else:
                        logging.info("No more 'Load More' buttons found")
                        break
                        
                except NoSuchElementException:
                    logging.info("All trips loaded for this week")
                    break
                except Exception as e:
                    logging.warning(f"Error clicking load more: {e}")
                    break
            
            return True
            
        except Exception as e:
            logging.error(f"Error loading all trips: {e}")
            return False
    
    def get_trip_links(self):
        """Extract all trip detail links from the current week"""
        try:
            # Look for trip rows and detail buttons
            trip_selectors = [
                "a[href*='/trips/']",
                "button[data-testid*='trip-details']",
                "//a[contains(@href, '/trips/')]",
                "//button[contains(@onclick, '/trips/')]"
            ]
            
            trip_links = []
            for selector in trip_selectors:
                try:
                    if selector.startswith("//"):
                        elements = self.driver.find_elements(By.XPATH, selector)
                    else:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for element in elements:
                        href = element.get_attribute('href') or element.get_attribute('onclick')
                        if href and '/trips/' in href:
                            # Extract trip ID from URL
                            trip_id = href.split('/trips/')[-1].split('/')[0].split('?')[0]
                            if trip_id and trip_id not in [t['id'] for t in trip_links]:
                                trip_links.append({
                                    'id': trip_id,
                                    'element': element,
                                    'url': f"https://drivers.uber.com/trips/{trip_id}"
                                })
                    
                    if trip_links:
                        break
                        
                except Exception:
                    continue
            
            logging.info(f"Found {len(trip_links)} trip links")
            return trip_links
            
        except Exception as e:
            logging.error(f"Error extracting trip links: {e}")
            return []
    
    def scrape_trip_details(self, trip_url, trip_id, week_start):
        """Scrape detailed trip information from trip detail page"""
        try:
            # Open trip in new tab
            original_window = self.driver.current_window_handle
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            self.driver.get(trip_url)
            
            # Wait for page to load
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            sleep(3)
            
            trip_data = {
                'trip_id': trip_id,
                'scraped_at': datetime.now().isoformat(),
                'week_start': week_start.isoformat(),
                'url': trip_url
            }
            
            # Extract basic trip information
            try:
                # Trip type and status
                type_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), 'Trip')]")
                if type_elements:
                    trip_data['trip_type'] = type_elements[0].text
            except:
                pass
            
            # Extract date and time
            try:
                date_elements = self.driver.find_elements(By.XPATH, "//*[contains(text(), '202')]")
                for element in date_elements:
                    text = element.text
                    if '202' in text and len(text) < 50:  # Simple date filter
                        trip_data['date_time'] = text
                        break
            except:
                pass
            
            # Extract earnings information
            earning_selectors = [
                "//*[contains(text(), 'Earnings')]",
                "//*[contains(text(), 'earning')]",
                "//*[contains(text(), '$')]"
            ]
            
            earnings_data = {}
            for selector in earning_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for element in elements:
                        text = element.text
                        if '$' in text:
                            # Simple parsing for earnings
                            lines = text.split('\n')
                            for line in lines:
                                if any(keyword in line.lower() for keyword in ['fare', 'service', 'tax', 'total', 'earning']):
                                    parts = line.split('$')
                                    if len(parts) > 1:
                                        key = parts[0].strip().lower().replace(' ', '_')
                                        value = parts[1].split()[0] if parts[1] else ''
                                        earnings_data[key] = f"${value}"
                except:
                    continue
            
            trip_data['earnings'] = earnings_data
            
            # Extract distance and duration
            try:
                metrics = self.driver.find_elements(By.XPATH, "//*[contains(text(), 'mi') or contains(text(), 'min') or contains(text(), 'km')]")
                for metric in metrics:
                    text = metric.text
                    if 'mi' in text or 'km' in text:
                        trip_data['distance'] = text
                    elif 'min' in text:
                        trip_data['duration'] = text
            except:
                pass
            
            # Extract map data if available
            try:
                map_elements = self.driver.find_elements(By.TAG_NAME, "img")
                for img in map_elements:
                    src = img.get_attribute('src') or ''
                    if 'map' in src.lower() or 'static' in src.lower():
                        trip_data['map_url'] = src
                        break
            except:
                pass
            
            # Close the tab and return to original window
            self.driver.close()
            self.driver.switch_to.window(original_window)
            
            return trip_data
            
        except Exception as e:
            logging.error(f"Error scraping trip {trip_id}: {e}")
            # Ensure we return to original window
            try:
                self.driver.close()
                self.driver.switch_to.window(original_window)
            except:
                pass
            return None
    
    def save_trip_data(self, trip_data):
        """Save trip data to encrypted database"""
        try:
            encrypted_data, data_hash = self._encrypt_data(trip_data)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO trips (trip_id, encrypted_data, data_hash, week_start)
                VALUES (?, ?, ?, ?)
            ''', (trip_data['trip_id'], encrypted_data, data_hash, 
                  trip_data['week_start'].split('T')[0] if 'T' in trip_data['week_start'] else trip_data['week_start']))
            
            conn.commit()
            conn.close()
            
            logging.info(f"Saved trip data for {trip_data['trip_id']}")
            return True
            
        except Exception as e:
            logging.error(f"Error saving trip data: {e}")
            return False
    
    def process_week(self, week_range):
        """Process all trips in a given week range"""
        logging.info(f"Processing week: {week_range['display']}")
        
        if not self.navigate_to_week(week_range['start']):
            return 0
        
        if not self.load_all_trips_in_week():
            logging.warning("Could not load all trips for week")
        
        trip_links = self.get_trip_links()
        trips_processed = 0
        
        for trip_link in trip_links:
            # Check if trip already exists in database
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT id FROM trips WHERE trip_id = ?', (trip_link['id'],))
            exists = cursor.fetchone()
            conn.close()
            
            if exists:
                logging.info(f"Trip {trip_link['id']} already exists, skipping")
                continue
            
            # Scrape trip details
            trip_data = self.scrape_trip_details(
                trip_link['url'], 
                trip_link['id'], 
                week_range['start']
            )
            
            if trip_data:
                if self.save_trip_data(trip_data):
                    trips_processed += 1
            
            # Be respectful to the server
            sleep(1)
        
        logging.info(f"Processed {trips_processed} new trips for week {week_range['display']}")
        return trips_processed
    
    def run_scraping_session(self, uber_url, weeks_to_process=4):
        """Main method to run the scraping session"""
        try:
            logging.info("Starting Uber data scraping session")
            
            # Get week ranges to process
            week_ranges = self.get_week_ranges(weeks_back=weeks_to_process)
            total_trips = 0
            
            for week_range in week_ranges:
                trips_processed = self.process_week(week_range)
                total_trips += trips_processed
                
                # Record session progress
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO scraping_sessions (weeks_processed, trips_collected)
                    VALUES (?, ?)
                ''', (1, trips_processed))
                conn.commit()
                conn.close()
            
            logging.info(f"Scraping session completed. Total trips collected: {total_trips}")
            return total_trips
            
        except Exception as e:
            logging.error(f"Scraping session failed: {e}")
            return 0
    
    def export_data(self, output_format='json', filename=None):
        """Export collected data in specified format"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT trip_id, encrypted_data, data_hash 
                FROM trips 
                ORDER BY week_start DESC, trip_id
            ''')
            
            trips = []
            for row in cursor.fetchall():
                trip_id, encrypted_data, data_hash = row
                trip_data = self._decrypt_data(encrypted_data, data_hash)
                if trip_data:
                    trips.append(trip_data)
            
            conn.close()
            
            if not filename:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f'uber_trips_export_{timestamp}.{output_format}'
            
            if output_format.lower() == 'json':
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(trips, f, indent=2, ensure_ascii=False)
            
            elif output_format.lower() == 'csv':
                # Simple CSV export - would need more sophisticated flattening for nested data
                import csv
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    if trips:
                        fieldnames = set()
                        for trip in trips:
                            fieldnames.update(trip.keys())
                        
                        writer = csv.DictWriter(f, fieldnames=sorted(fieldnames))
                        writer.writeheader()
                        writer.writerows(trips)
            
            logging.info(f"Data exported to {filename}")
            return filename
            
        except Exception as e:
            logging.error(f"Export failed: {e}")
            return None
    
    def close(self):
        """Close the browser"""
        if self.driver:
            self.driver.quit()
            logging.info("Browser closed")

def main():
    """Main execution function"""
    scraper = None
    try:
        # Configuration
        UBER_ACTIVITIES_URL = "https://drivers.uber.com/p3/payments/activities"  # Update with actual URL
        BRAVE_BROWSER_PATH = "/usr/bin/brave-browser"  # Update for your system
        BRAVE_PROFILE_PATH = None  # Will use default if None
        WEEKS_TO_PROCESS = 4  # Adjust based on needs
        
        # Initialize scraper
        scraper = UberDataScraper()
        
        # Setup browser
        scraper.setup_browser(BRAVE_BROWSER_PATH, BRAVE_PROFILE_PATH)
        
        # Run scraping session
        total_trips = scraper.run_scraping_session(UBER_ACTIVITIES_URL, WEEKS_TO_PROCESS)
        
        # Export data
        if total_trips > 0:
            scraper.export_data('json')
            scraper.export_data('csv')
        
        print(f"Scraping completed! Collected {total_trips} trips.")
        
    except KeyboardInterrupt:
        print("\nScraping interrupted by user")
    except Exception as e:
        logging.error(f"Main execution error: {e}")
    finally:
        if scraper:
            scraper.close()

if __name__ == "__main__":
    main()