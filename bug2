import asyncio
import os
import json
import hashlib
import hmac
import base64
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
from playwright.async_api import async_playwright, Page, Browser, BrowserContext
import sqlite3
from dataclasses import dataclass
import hashlib

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('uber_scraper.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class WeekRange:
    start: datetime
    end: datetime
    display: str

@dataclass
class TripLink:
    trip_id: str
    url: str

@dataclass
class TripData:
    trip_id: str
    scraped_at: str
    week_start: str
    url: str
    trip_type: Optional[str] = None
    date_time: Optional[str] = None
    earnings: Optional[Dict] = None
    distance: Optional[str] = None
    duration: Optional[str] = None
    map_url: Optional[str] = None
    fare_breakdown: Optional[Dict] = None

class UberDataScraper:
    def __init__(self, encryption_key: Optional[str] = None):
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.encryption_key = encryption_key or self._generate_encryption_key()
        self.db_path = 'uber_trips_data.db'
        self._init_database()
        
    def _generate_encryption_key(self) -> str:
        """Generate a secure encryption key"""
        return base64.b64encode(os.urandom(32)).decode('utf-8')
    
    def _init_database(self):
        """Initialize SQLite database with encrypted storage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS trips (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                trip_id TEXT UNIQUE,
                encrypted_data BLOB,
                data_hash TEXT,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                week_start DATE
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scraping_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                weeks_processed INTEGER,
                trips_collected INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _encrypt_data(self, data: Dict) -> tuple[str, str]:
        """Encrypt trip data using HMAC for integrity and simple obfuscation"""
        json_data = json.dumps(data, ensure_ascii=False)
        data_hash = hmac.new(
            self.encryption_key.encode('utf-8'),
            json_data.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()
        
        # Simple base64 encoding for obfuscation
        encrypted = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')
        return encrypted, data_hash
    
    def _decrypt_data(self, encrypted_data: str, expected_hash: str) -> Optional[Dict]:
        """Decrypt and verify data integrity"""
        try:
            json_data = base64.b64decode(encrypted_data.encode('utf-8')).decode('utf-8')
            
            # Verify integrity
            actual_hash = hmac.new(
                self.encryption_key.encode('utf-8'),
                json_data.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            
            if actual_hash != expected_hash:
                raise ValueError("Data integrity check failed")
                
            return json.loads(json_data)
        except Exception as e:
            logging.error(f"Decryption error: {e}")
            return None

    async def setup_browser(self, brave_path: Optional[str] = None, profile_path: Optional[str] = None):
        """Set up Playwright with Brave browser"""
        self.playwright = await async_playwright().start()
        
        # Launch browser with options
        launch_options = {
            "headless": False,
            "args": [
                "--no-first-run",
                "--no-default-browser-check",
                "--disable-blink-features=AutomationControlled",
            ]
        }
        
        # Use Brave if path provided
        if brave_path and os.path.exists(brave_path):
            launch_options["executable_path"] = brave_path
        
        self.browser = await self.playwright.chromium.launch(**launch_options)
        
        # Create context with existing profile if specified
        context_options = {}
        if profile_path and os.path.exists(profile_path):
            context_options["user_data_dir"] = profile_path
        
        self.context = await self.browser.new_context(**context_options)
        self.page = await self.context.new_page()
        
        # Stealth: remove webdriver property
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        """)
        
        logging.info("Browser setup completed")

    async def navigate_to_activities(self, uber_url: str) -> bool:
        """Navigate to Uber driver activities page"""
        try:
            await self.page.goto(uber_url, wait_until="networkidle")
            await self.page.wait_for_selector("body", timeout=15000)
            logging.info("Successfully navigated to Uber activities page")
            await asyncio.sleep(3)  # Allow page to fully load
            return True
        except Exception as e:
            logging.error(f"Failed to navigate to activities page: {e}")
            return False

    def get_week_ranges(self, start_date: Optional[datetime] = None, weeks_back: int = 52) -> List[WeekRange]:
        """Generate week ranges (Monday 4am to next Monday 3:59am)"""
        if not start_date:
            start_date = datetime.now()
        
        week_ranges = []
        for i in range(weeks_back):
            # Find most recent Monday
            days_since_monday = (start_date.weekday() - 0) % 7
            week_end = start_date - timedelta(days=days_since_monday)
            week_end = week_end.replace(hour=3, minute=59, second=59, microsecond=999999)
            week_start = week_end - timedelta(days=6, hours=23, minutes=59, seconds=59)
            
            week_ranges.append(WeekRange(
                start=week_start,
                end=week_end,
                display=week_start.strftime('%Y-%m-%d')
            ))
            
            start_date = week_start - timedelta(seconds=1)
        
        return week_ranges

    async def navigate_to_week(self, week_date: datetime) -> bool:
        """Navigate to specific week using calendar"""
        try:
            # Look for calendar element with multiple selector strategies
            calendar_selectors = [
                "input[type='date']",
                "input[type='text'][placeholder*='date']",
                "[data-testid*='date']",
                ".calendar-input",
                "input.calendar"
            ]
            
            for selector in calendar_selectors:
                try:
                    calendar_input = await self.page.wait_for_selector(selector, timeout=5000)
                    if calendar_input:
                        await calendar_input.click()
                        await calendar_input.fill("")
                        await calendar_input.type(week_date.strftime('%Y-%m-%d'))
                        await self.page.keyboard.press("Enter")
                        
                        # Wait for page to load new data
                        await self.page.wait_for_load_state("networkidle")
                        await asyncio.sleep(3)
                        
                        logging.info(f"Navigated to week starting {week_date.strftime('%Y-%m-%d')}")
                        return True
                except Exception:
                    continue
            
            logging.error(f"Could not find calendar input for week {week_date}")
            return False
            
        except Exception as e:
            logging.error(f"Failed to navigate to week {week_date}: {e}")
            return False

    async def load_all_trips_in_week(self) -> bool:
        """Click 'Load More' button until all trips are loaded"""
        try:
            load_more_clicks = 0
            max_clicks = 50  # Safety limit
            
            while load_more_clicks < max_clicks:
                # Try multiple selector strategies for load more button
                load_more_selectors = [
                    "button:has-text('Load More')",
                    "button:has-text('Load more')",
                    "[data-testid*='load-more']",
                    "[aria-label*='load more']",
                    ".load-more",
                    "button.load-more"
                ]
                
                load_more_button = None
                for selector in load_more_selectors:
                    try:
                        load_more_button = await self.page.wait_for_selector(selector, timeout=2000)
                        if load_more_button:
                            break
                    except Exception:
                        continue
                
                if load_more_button:
                    # Check if button is visible and enabled
                    is_visible = await load_more_button.is_visible()
                    is_disabled = await load_more_button.get_attribute("disabled")
                    
                    if is_visible and not is_disabled:
                        await load_more_button.click()
                        load_more_clicks += 1
                        logging.info(f"Clicked 'Load More' button ({load_more_clicks})")
                        
                        # Wait for content to load
                        await self.page.wait_for_load_state("networkidle")
                        await asyncio.sleep(2)
                    else:
                        logging.info("Load more button is not clickable")
                        break
                else:
                    logging.info("No more 'Load More' buttons found")
                    break
            
            return True
            
        except Exception as e:
            logging.error(f"Error loading all trips: {e}")
            return False

    async def get_trip_links(self) -> List[TripLink]:
        """Extract all trip detail links from the current week"""
        try:
            # Look for trip rows and detail buttons with multiple strategies
            trip_selectors = [
                "a[href*='/trips/']",
                "button[data-testid*='trip-details']",
                "[href*='/trips/']",
                ".trip-link",
                "a.trip-details"
            ]
            
            trip_links = []
            for selector in trip_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    
                    for element in elements:
                        href = await element.get_attribute('href')
                        if href and '/trips/' in href:
                            # Extract trip ID from URL
                            trip_id = href.split('/trips/')[-1].split('/')[0].split('?')[0]
                            if trip_id and trip_id not in [t.trip_id for t in trip_links]:
                                full_url = f"https://drivers.uber.com/trips/{trip_id}" if not href.startswith('http') else href
                                trip_links.append(TripLink(
                                    trip_id=trip_id,
                                    url=full_url
                                ))
                    
                    if trip_links:
                        break
                        
                except Exception as e:
                    logging.debug(f"Selector {selector} failed: {e}")
                    continue
            
            # Also try to find trip IDs in data attributes
            if not trip_links:
                try:
                    trip_elements = await self.page.query_selector_all('[data-trip-id], [id*="trip"]')
                    for element in trip_elements:
                        trip_id = await element.get_attribute('data-trip-id')
                        if trip_id:
                            trip_links.append(TripLink(
                                trip_id=trip_id,
                                url=f"https://drivers.uber.com/trips/{trip_id}"
                            ))
                except Exception:
                    pass
            
            logging.info(f"Found {len(trip_links)} trip links")
            return trip_links
            
        except Exception as e:
            logging.error(f"Error extracting trip links: {e}")
            return []

    async def scrape_trip_details(self, trip_url: str, trip_id: str, week_start: datetime) -> Optional[TripData]:
        """Scrape detailed trip information from trip detail page"""
        try:
            # Create a new page for the trip details
            trip_page = await self.context.new_page()
            
            # Navigate to trip details
            await trip_page.goto(trip_url, wait_until="networkidle")
            await trip_page.wait_for_selector("body", timeout=15000)
            await asyncio.sleep(3)
            
            trip_data = TripData(
                trip_id=trip_id,
                scraped_at=datetime.now().isoformat(),
                week_start=week_start.isoformat(),
                url=trip_url
            )
            
            # Extract basic trip information with robust selectors
            try:
                # Trip type and status
                type_selectors = [
                    "h1, h2, h3",
                    "[data-testid*='trip-type']",
                    ".trip-type",
                    ".trip-header"
                ]
                
                for selector in type_selectors:
                    elements = await trip_page.query_selector_all(selector)
                    for element in elements:
                        text = await element.text_content()
                        if text and ('trip' in text.lower() or 'ride' in text.lower()):
                            trip_data.trip_type = text.strip()
                            break
                    if trip_data.trip_type:
                        break
            except Exception as e:
                logging.debug(f"Could not extract trip type: {e}")
            
            # Extract date and time
            try:
                date_selectors = [
                    "[data-testid*='date']",
                    "[data-testid*='time']",
                    ".trip-date",
                    ".trip-time",
                    "time"
                ]
                
                for selector in date_selectors:
                    elements = await trip_page.query_selector_all(selector)
                    for element in elements:
                        text = await element.text_content()
                        if text and ('202' in text or 'pm' in text.lower() or 'am' in text.lower()):
                            trip_data.date_time = text.strip()
                            break
                    if trip_data.date_time:
                        break
            except Exception as e:
                logging.debug(f"Could not extract date/time: {e}")
            
            # Extract earnings information
            earnings_data = {}
            try:
                earning_selectors = [
                    "[data-testid*='earnings']",
                    "[data-testid*='fare']",
                    ".earnings",
                    ".fare-breakdown",
                    "table"
                ]
                
                for selector in earning_selectors:
                    elements = await trip_page.query_selector_all(selector)
                    for element in elements:
                        text = await element.text_content()
                        if text and '$' in text:
                            # Parse earnings table or section
                            lines = text.split('\n')
                            for line in lines:
                                line = line.strip()
                                if any(keyword in line.lower() for keyword in 
                                      ['fare', 'service', 'tax', 'total', 'earning', 'payout']):
                                    parts = line.split('$')
                                    if len(parts) > 1:
                                        key = parts[0].strip().lower().replace(' ', '_')
                                        value = parts[1].split()[0] if parts[1] else ''
                                        earnings_data[key] = f"${value}"
            except Exception as e:
                logging.debug(f"Could not extract earnings: {e}")
            
            trip_data.earnings = earnings_data
            
            # Extract distance and duration
            try:
                metric_selectors = [
                    "[data-testid*='distance']",
                    "[data-testid*='duration']",
                    ".trip-distance",
                    ".trip-duration"
                ]
                
                for selector in metric_selectors:
                    elements = await trip_page.query_selector_all(selector)
                    for element in elements:
                        text = await element.text_content()
                        if text:
                            if 'mi' in text.lower() or 'km' in text.lower():
                                trip_data.distance = text.strip()
                            elif 'min' in text.lower():
                                trip_data.duration = text.strip()
            except Exception as e:
                logging.debug(f"Could not extract metrics: {e}")
            
            # Extract map data if available
            try:
                map_selectors = [
                    "img[src*='map']",
                    "img[alt*='map']",
                    "img[src*='static']",
                    ".map-image"
                ]
                
                for selector in map_selectors:
                    map_img = await trip_page.query_selector(selector)
                    if map_img:
                        src = await map_img.get_attribute('src')
                        if src:
                            trip_data.map_url = src
                            break
            except Exception as e:
                logging.debug(f"Could not extract map: {e}")
            
            # Close the trip page
            await trip_page.close()
            
            logging.info(f"Successfully scraped trip {trip_id}")
            return trip_data
            
        except Exception as e:
            logging.error(f"Error scraping trip {trip_id}: {e}")
            # Ensure we close the trip page
            try:
                await trip_page.close()
            except:
                pass
            return None

    def save_trip_data(self, trip_data: TripData) -> bool:
        """Save trip data to encrypted database"""
        try:
            # Convert dataclass to dict
            data_dict = {
                'trip_id': trip_data.trip_id,
                'scraped_at': trip_data.scraped_at,
                'week_start': trip_data.week_start,
                'url': trip_data.url,
                'trip_type': trip_data.trip_type,
                'date_time': trip_data.date_time,
                'earnings': trip_data.earnings,
                'distance': trip_data.distance,
                'duration': trip_data.duration,
                'map_url': trip_data.map_url
            }
            
            encrypted_data, data_hash = self._encrypt_data(data_dict)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO trips (trip_id, encrypted_data, data_hash, week_start)
                VALUES (?, ?, ?, ?)
            ''', (trip_data.trip_id, encrypted_data, data_hash, 
                  trip_data.week_start.split('T')[0] if 'T' in trip_data.week_start else trip_data.week_start))
            
            conn.commit()
            conn.close()
            
            logging.info(f"Saved trip data for {trip_data.trip_id}")
            return True
            
        except Exception as e:
            logging.error(f"Error saving trip data: {e}")
            return False

    async def process_week(self, week_range: WeekRange) -> int:
        """Process all trips in a given week range"""
        logging.info(f"Processing week: {week_range.display}")
        
        if not await self.navigate_to_week(week_range.start):
            return 0
        
        if not await self.load_all_trips_in_week():
            logging.warning("Could not load all trips for week")
        
        trip_links = await self.get_trip_links()
        trips_processed = 0
        
        for trip_link in trip_links:
            # Check if trip already exists in database
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT id FROM trips WHERE trip_id = ?', (trip_link.trip_id,))
            exists = cursor.fetchone()
            conn.close()
            
            if exists:
                logging.info(f"Trip {trip_link.trip_id} already exists, skipping")
                continue
            
            # Scrape trip details
            trip_data = await self.scrape_trip_details(
                trip_link.url, 
                trip_link.trip_id, 
                week_range.start
            )
            
            if trip_data:
                if self.save_trip_data(trip_data):
                    trips_processed += 1
            
            # Be respectful to the server
            await asyncio.sleep(1)
        
        logging.info(f"Processed {trips_processed} new trips for week {week_range.display}")
        return trips_processed

    async def run_scraping_session(self, uber_url: str, weeks_to_process: int = 4) -> int:
        """Main method to run the scraping session"""
        try:
            logging.info("Starting Uber data scraping session")
            
            # Navigate to activities page first
            if not await self.navigate_to_activities(uber_url):
                logging.error("Failed to navigate to activities page")
                return 0
            
            # Get week ranges to process
            week_ranges = self.get_week_ranges(weeks_back=weeks_to_process)
            total_trips = 0
            
            for week_range in week_ranges:
                trips_processed = await self.process_week(week_range)
                total_trips += trips_processed
                
                # Record session progress
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO scraping_sessions (weeks_processed, trips_collected)
                    VALUES (?, ?)
                ''', (1, trips_processed))
                conn.commit()
                conn.close()
            
            logging.info(f"Scraping session completed. Total trips collected: {total_trips}")
            return total_trips
            
        except Exception as e:
            logging.error(f"Scraping session failed: {e}")
            return 0

    def export_data(self, output_format: str = 'json', filename: Optional[str] = None) -> Optional[str]:
        """Export collected data in specified format"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT trip_id, encrypted_data, data_hash 
                FROM trips 
                ORDER BY week_start DESC, trip_id
            ''')
            
            trips = []
            for row in cursor.fetchall():
                trip_id, encrypted_data, data_hash = row
                trip_data = self._decrypt_data(encrypted_data, data_hash)
                if trip_data:
                    trips.append(trip_data)
            
            conn.close()
            
            if not filename:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f'uber_trips_export_{timestamp}.{output_format}'
            
            if output_format.lower() == 'json':
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(trips, f, indent=2, ensure_ascii=False)
            
            elif output_format.lower() == 'csv':
                import csv
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    if trips:
                        fieldnames = set()
                        for trip in trips:
                            fieldnames.update(trip.keys())
                        
                        writer = csv.DictWriter(f, fieldnames=sorted(fieldnames))
                        writer.writeheader()
                        writer.writerows(trips)
            
            logging.info(f"Data exported to {filename}")
            return filename
            
        except Exception as e:
            logging.error(f"Export failed: {e}")
            return None

    async def close(self):
        """Close the browser and cleanup"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        logging.info("Browser closed and resources cleaned up")

async def main():
    """Main execution function"""
    scraper = None
    try:
        # Configuration
        UBER_ACTIVITIES_URL = "https://drivers.uber.com/p3/payments/activities"  # Update with actual URL
        BRAVE_BROWSER_PATH = "/usr/bin/brave-browser"  # Update for your system
        BRAVE_PROFILE_PATH = None  # Will use default if None
        WEEKS_TO_PROCESS = 4  # Adjust based on needs
        
        # Initialize scraper
        scraper = UberDataScraper()
        
        # Setup browser
        await scraper.setup_browser(BRAVE_BROWSER_PATH, BRAVE_PROFILE_PATH)
        
        # Run scraping session
        total_trips = await scraper.run_scraping_session(UBER_ACTIVITIES_URL, WEEKS_TO_PROCESS)
        
        # Export data
        if total_trips > 0:
            scraper.export_data('json')
            scraper.export_data('csv')
        
        print(f"Scraping completed! Collected {total_trips} trips.")
        
    except KeyboardInterrupt:
        print("\nScraping interrupted by user")
    except Exception as e:
        logging.error(f"Main execution error: {e}")
    finally:
        if scraper:
            await scraper.close()

if __name__ == "__main__":
    asyncio.run(main())